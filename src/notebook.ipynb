{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openml\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data_list.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam' from 'optimizers' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Adam' from 'optimizers' (unknown location)"
     ]
    }
   ],
   "source": [
    "from lr import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "optimizer = Adam(learning_rate=0.0001, beta1=0.9, beta2=0.999)\n",
    "model = LogisticRegression(optimizer=optimizer, epochs=400)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model = LR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small datasets:\n",
    "- 37: diabetes\n",
    "- 1462: banknote-authentication\n",
    "- 871: pollen\n",
    "\n",
    "Large datasets:\n",
    "- 752: puma32H\n",
    "- 1120: MagicTelescope\n",
    "- 23512: higgs\n",
    "- 23517: numerai28.6\n",
    "- 979: waveform-5000\n",
    "- 1487: ozone-level-8hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(id):\n",
    "    dataset = openml.datasets.get_dataset(id)\n",
    "    df, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "    numerical_cols = df.select_dtypes(include='number').columns\n",
    "    target_col = df.select_dtypes(exclude='number').columns\n",
    "    X = df[numerical_cols].to_numpy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df[target_col])\n",
    "    return X, y\n",
    "\n",
    "def list_data(id_list):\n",
    "    data_list = []\n",
    "    for id in id_list:\n",
    "        data_list.append(read_data(id))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [37, 1462, 871, 752, 1120, 23512, 23517, 979, 1487] #small datasets first\n",
    "data_list = list_data(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 has no missing values.\n",
      "Dataset 2 has no missing values.\n",
      "Dataset 3 has no missing values.\n",
      "Dataset 4 has no missing values.\n",
      "Dataset 5 has no missing values.\n",
      "Dataset 6 has missing values:\n",
      "Missing values in X: 9\n",
      "Missing values in y: 0\n",
      "Dataset 7 has no missing values.\n",
      "Dataset 8 has no missing values.\n",
      "Dataset 9 has no missing values.\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(data_list):\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    missing_X = X.isnull().sum().sum()\n",
    "    missing_y = y.isnull().sum().sum()\n",
    "    \n",
    "    if missing_X > 0 or missing_y > 0:\n",
    "        print(f\"Dataset {i+1} has missing values:\")\n",
    "        print(f\"Missing values in X: {missing_X}\")\n",
    "        print(f\"Missing values in y: {missing_y}\")\n",
    "        X = X.fillna(X.mean())\n",
    "        y = y.fillna(y.mean())\n",
    "        data_list[i] = (X.to_numpy(), (y.to_numpy()).flatten())\n",
    "    else:\n",
    "        print(f\"Dataset {i+1} has no missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df, threshold=0.8):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset 1 for highly correlated columns...\n",
      "0 highly correlated columns removed.\n",
      "Checking dataset 2 for highly correlated columns...\n",
      "0 highly correlated columns removed.\n",
      "Checking dataset 3 for highly correlated columns...\n",
      "1 highly correlated columns removed.\n",
      "Checking dataset 4 for highly correlated columns...\n",
      "0 highly correlated columns removed.\n",
      "Checking dataset 5 for highly correlated columns...\n",
      "2 highly correlated columns removed.\n",
      "Checking dataset 6 for highly correlated columns...\n",
      "1 highly correlated columns removed.\n",
      "Checking dataset 7 for highly correlated columns...\n",
      "8 highly correlated columns removed.\n",
      "Checking dataset 8 for highly correlated columns...\n",
      "0 highly correlated columns removed.\n",
      "Checking dataset 9 for highly correlated columns...\n",
      "57 highly correlated columns removed.\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(data_list):\n",
    "    print(f\"Checking dataset {i+1} for highly correlated columns...\")\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y, columns=['y'])\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "\n",
    "    X_cleaned = remove_cols(data.drop(columns=['y']), threshold=0.8)\n",
    "    y_cleaned = data['y']\n",
    "    \n",
    "    data_list[i] = (X_cleaned.to_numpy(), (y_cleaned.to_numpy()).flatten())\n",
    "    print(f\"{X.shape[1] - X_cleaned.shape[1]} highly correlated columns removed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
